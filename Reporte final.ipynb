{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por refuerzo para entrenar una Red Neuronal para jugar Atari\n",
    "## Proyecto final de Teoría del Cerebro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timothee Givois, Alfredo Carrillo, Didier Muñoz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto el objetivo es enseñar a una red neuronal a aprender a jugar un videojuego de Atari.\n",
    "\n",
    "Para esto se utilizaron las siguientes librerías:\n",
    "\n",
    "* gym: para simular el videojuego de Atari.\n",
    "* tensorflow: para construir la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable *env* es el objeto creado por gym para simular el videojuego.\n",
    "El videojuego escogido fue Breakout. Este videojuego tiene 4 acciones posibles y cada observación consiste en la imagen de 210 x 160 pixeles del juego en el momento observado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(4)\n",
      "State Space Box(210, 160, 3)\n",
      "Action Meaning ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "env.reset()\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "print(\"Action Meaning {}\".format(env.unwrapped.get_action_meanings()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la imagen inicial del juego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image(board):    \n",
    "    plt.imshow(board)\n",
    "    plt.show()\n",
    "\n",
    "show_image(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte importante para simplificar un poco el problema, es el preprocesamiento de las imágenes. \n",
    "Se construyó la función *preprocess()* para realizar este proceso que consta de lo siguiente:\n",
    "* Recortar la imagen observada del juego para quitar las partes que no nos interesan (números en la parte superior y marco de la imagen.\n",
    "* Reducir el tamaño de la imagen.\n",
    "* Cambiar la imagen a escala de grises.\n",
    "\n",
    "La imagen inicial del juego ya preprocesada se ve así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(82, 75, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def preprocess(image):\n",
    "    # crop and downsample image\n",
    "    new_image = image[31:195:2, 5:155:2]\n",
    "    # change image colors\n",
    "    #new_image = rgb2gray(new_image)\n",
    "    # walls should be 142 in rgb and points should be 74\n",
    "    new_image[np.logical_and(new_image[:,:] != 0, new_image[:,:]!=142)] = 74\n",
    "    \n",
    "    return new_image\n",
    "\n",
    "preprocessed = preprocess(env.reset())\n",
    "show_image(preprocessed)\n",
    "np.shape(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de comparación se hizo un modelo dummy con elecciones aleatorias para ver el puntaje que se puede lograr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game: 0/99, actions_made: 287, score: 2.0\n",
      "game: 1/99, actions_made: 237, score: 1.0\n",
      "game: 2/99, actions_made: 361, score: 4.0\n",
      "game: 3/99, actions_made: 243, score: 1.0\n",
      "game: 4/99, actions_made: 247, score: 1.0\n",
      "game: 5/99, actions_made: 171, score: 0.0\n",
      "game: 6/99, actions_made: 278, score: 2.0\n",
      "game: 7/99, actions_made: 172, score: 0.0\n",
      "game: 8/99, actions_made: 174, score: 0.0\n",
      "game: 9/99, actions_made: 169, score: 0.0\n",
      "game: 10/99, actions_made: 182, score: 0.0\n",
      "game: 11/99, actions_made: 269, score: 2.0\n",
      "game: 12/99, actions_made: 365, score: 3.0\n",
      "game: 13/99, actions_made: 202, score: 0.0\n",
      "game: 14/99, actions_made: 219, score: 1.0\n",
      "game: 15/99, actions_made: 238, score: 1.0\n",
      "game: 16/99, actions_made: 315, score: 3.0\n",
      "game: 17/99, actions_made: 300, score: 3.0\n",
      "game: 18/99, actions_made: 167, score: 0.0\n",
      "game: 19/99, actions_made: 319, score: 3.0\n",
      "game: 20/99, actions_made: 181, score: 0.0\n",
      "game: 21/99, actions_made: 236, score: 1.0\n",
      "game: 22/99, actions_made: 221, score: 1.0\n",
      "game: 23/99, actions_made: 268, score: 2.0\n",
      "game: 24/99, actions_made: 409, score: 4.0\n",
      "game: 25/99, actions_made: 243, score: 1.0\n",
      "game: 26/99, actions_made: 228, score: 1.0\n",
      "game: 27/99, actions_made: 261, score: 1.0\n",
      "game: 28/99, actions_made: 329, score: 3.0\n",
      "game: 29/99, actions_made: 264, score: 2.0\n",
      "game: 30/99, actions_made: 275, score: 2.0\n",
      "game: 31/99, actions_made: 180, score: 0.0\n",
      "game: 32/99, actions_made: 348, score: 3.0\n",
      "game: 33/99, actions_made: 164, score: 0.0\n",
      "game: 34/99, actions_made: 234, score: 1.0\n",
      "game: 35/99, actions_made: 304, score: 3.0\n",
      "game: 36/99, actions_made: 252, score: 1.0\n",
      "game: 37/99, actions_made: 271, score: 2.0\n",
      "game: 38/99, actions_made: 241, score: 1.0\n",
      "game: 39/99, actions_made: 231, score: 1.0\n",
      "game: 40/99, actions_made: 281, score: 2.0\n",
      "game: 41/99, actions_made: 169, score: 0.0\n",
      "game: 42/99, actions_made: 380, score: 4.0\n",
      "game: 43/99, actions_made: 178, score: 0.0\n",
      "game: 44/99, actions_made: 277, score: 2.0\n",
      "game: 45/99, actions_made: 237, score: 1.0\n",
      "game: 46/99, actions_made: 176, score: 0.0\n",
      "game: 47/99, actions_made: 218, score: 1.0\n",
      "game: 48/99, actions_made: 172, score: 0.0\n",
      "game: 49/99, actions_made: 361, score: 3.0\n",
      "game: 50/99, actions_made: 216, score: 1.0\n",
      "game: 51/99, actions_made: 216, score: 1.0\n",
      "game: 52/99, actions_made: 169, score: 0.0\n",
      "game: 53/99, actions_made: 344, score: 3.0\n",
      "game: 54/99, actions_made: 283, score: 2.0\n",
      "game: 55/99, actions_made: 183, score: 0.0\n",
      "game: 56/99, actions_made: 337, score: 3.0\n",
      "game: 57/99, actions_made: 370, score: 4.0\n",
      "game: 58/99, actions_made: 250, score: 1.0\n",
      "game: 59/99, actions_made: 413, score: 4.0\n",
      "game: 60/99, actions_made: 171, score: 0.0\n",
      "game: 61/99, actions_made: 299, score: 2.0\n",
      "game: 62/99, actions_made: 199, score: 0.0\n",
      "game: 63/99, actions_made: 305, score: 2.0\n",
      "game: 64/99, actions_made: 333, score: 3.0\n",
      "game: 65/99, actions_made: 289, score: 2.0\n",
      "game: 66/99, actions_made: 241, score: 1.0\n",
      "game: 67/99, actions_made: 354, score: 4.0\n",
      "game: 68/99, actions_made: 250, score: 2.0\n",
      "game: 69/99, actions_made: 171, score: 0.0\n",
      "game: 70/99, actions_made: 243, score: 1.0\n",
      "game: 71/99, actions_made: 272, score: 2.0\n",
      "game: 72/99, actions_made: 265, score: 2.0\n",
      "game: 73/99, actions_made: 262, score: 2.0\n",
      "game: 74/99, actions_made: 183, score: 0.0\n",
      "game: 75/99, actions_made: 164, score: 0.0\n",
      "game: 76/99, actions_made: 177, score: 0.0\n",
      "game: 77/99, actions_made: 296, score: 3.0\n",
      "game: 78/99, actions_made: 369, score: 3.0\n",
      "game: 79/99, actions_made: 277, score: 2.0\n",
      "game: 80/99, actions_made: 168, score: 0.0\n",
      "game: 81/99, actions_made: 259, score: 2.0\n",
      "game: 82/99, actions_made: 201, score: 1.0\n",
      "game: 83/99, actions_made: 273, score: 2.0\n",
      "game: 84/99, actions_made: 602, score: 7.0\n",
      "game: 85/99, actions_made: 297, score: 2.0\n",
      "game: 86/99, actions_made: 244, score: 2.0\n",
      "game: 87/99, actions_made: 215, score: 1.0\n",
      "game: 88/99, actions_made: 307, score: 3.0\n",
      "game: 89/99, actions_made: 451, score: 5.0\n",
      "game: 90/99, actions_made: 172, score: 0.0\n",
      "game: 91/99, actions_made: 253, score: 1.0\n",
      "game: 92/99, actions_made: 318, score: 3.0\n",
      "game: 93/99, actions_made: 340, score: 3.0\n",
      "game: 94/99, actions_made: 218, score: 1.0\n",
      "game: 95/99, actions_made: 214, score: 1.0\n",
      "game: 96/99, actions_made: 186, score: 0.0\n",
      "game: 97/99, actions_made: 179, score: 0.0\n",
      "game: 98/99, actions_made: 278, score: 2.0\n"
     ]
    }
   ],
   "source": [
    "class DummyModel:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "    def run(self, games, env):\n",
    "\n",
    "        for game in range(games):\n",
    "            total_reward = 0\n",
    "            actions_made = 0\n",
    "            env.reset()\n",
    "            while True:\n",
    "                actions_made += 1\n",
    "                action = np.random.choice(self.actions)\n",
    "                state, reward, done, _  = env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    print('game: {}/{}, actions_made: {}, score: {}'.format(game, games, actions_made, total_reward))\n",
    "                    break\n",
    "   \n",
    "\n",
    "dummy_model = DummyModel([i for i in range(0, 4)])\n",
    "dummy_model.run(99, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que se lograron puntajes de a lo más 4, y juegos de longitud 421 pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Model.py se encuentra el modelo que entrenamos.\n",
    "Para entrenar la red, la pusimos a jugar 10000 veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Model.py\n",
    "\n",
    "modelo = Model(env.unwrapped.get_action_meanings())\n",
    "modelo.run(10000, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game: 0/10000, actions_made: 408, score: 4.0\n",
    "game: 1/10000, actions_made: 174, score: 0.0\n",
    "game: 2/10000, actions_made: 411, score: 4.0\n",
    "game: 3/10000, actions_made: 209, score: 1.0\n",
    "game: 4/10000, actions_made: 220, score: 1.0\n",
    "game: 5/10000, actions_made: 234, score: 1.0\n",
    "game: 6/10000, actions_made: 315, score: 2.0\n",
    "game: 7/10000, actions_made: 228, score: 1.0\n",
    "game: 8/10000, actions_made: 321, score: 3.0\n",
    "game: 9/10000, actions_made: 212, score: 1.0\n",
    "game: 10/10000, actions_made: 173, score: 0.0\n",
    "game: 11/10000, actions_made: 295, score: 2.0\n",
    "game: 12/10000, actions_made: 414, score: 4.0\n",
    "game: 13/10000, actions_made: 195, score: 0.0\n",
    "game: 14/10000, actions_made: 240, score: 1.0\n",
    "game: 15/10000, actions_made: 173, score: 0.0\n",
    "game: 16/10000, actions_made: 177, score: 0.0\n",
    "game: 17/10000, actions_made: 298, score: 2.0\n",
    "game: 18/10000, actions_made: 181, score: 0.0\n",
    "game: 19/10000, actions_made: 195, score: 0.0\n",
    "game: 20/10000, actions_made: 164, score: 0.0\n",
    "...\n",
    "game: 4778/10000, actions_made: 1996, score: 0.0\n",
    "game: 4781/10000, actions_made: 1943, score: 10.0\n",
    "game: 4786/10000, actions_made: 1950, score: 0.0\n",
    "game: 4793/10000, actions_made: 1124, score: 0.0\n",
    "game: 4794/10000, actions_made: 732, score: 0.0\n",
    "game: 4797/10000, actions_made: 1217, score: 2.0\n",
    "game: 4800/10000, actions_made: 886, score: 0.0\n",
    "game: 4807/10000, actions_made: 1566, score: 0.0\n",
    "game: 4812/10000, actions_made: 1711, score: 0.0\n",
    "game: 4817/10000, actions_made: 1919, score: 1.0\n",
    "game: 4819/10000, actions_made: 1699, score: 0.0\n",
    "game: 4827/10000, actions_made: 1473, score: 0.0\n",
    "game: 4828/10000, actions_made: 1346, score: 4.0\n",
    "game: 4831/10000, actions_made: 1856, score: 0.0\n",
    "game: 4832/10000, actions_made: 1942, score: 3.0\n",
    "game: 4841/10000, actions_made: 1041, score: 1.0\n",
    "game: 4854/10000, actions_made: 1695, score: 1.0\n",
    "...\n",
    "game: 4708/10000, actions_made: 1994, score: 0.0\n",
    "game: 4712/10000, actions_made: 1549, score: 0.0\n",
    "game: 4713/10000, actions_made: 1699, score: 0.0\n",
    "game: 4717/10000, actions_made: 1543, score: 1.0\n",
    "game: 4727/10000, actions_made: 1229, score: 0.0\n",
    "game: 4729/10000, actions_made: 1447, score: 2.0\n",
    "game: 4736/10000, actions_made: 945, score: 0.0\n",
    "game: 4740/10000, actions_made: 447, score: 0.0\n",
    "game: 4745/10000, actions_made: 1470, score: 0.0\n",
    "game: 4748/10000, actions_made: 1722, score: 0.0\n",
    "game: 4752/10000, actions_made: 1558, score: 0.0\n",
    "game: 4753/10000, actions_made: 1765, score: 2.0\n",
    "game: 4755/10000, actions_made: 1496, score: 0.0\n",
    "game: 4756/10000, actions_made: 1072, score: 0.0\n",
    "game: 4757/10000, actions_made: 1336, score: 0.0\n",
    "game: 4758/10000, actions_made: 1832, score: 2.0\n",
    "game: 4761/10000, actions_made: 1306, score: 2.0\n",
    "game: 4766/10000, actions_made: 1240, score: 0.0\n",
    "game: 4774/10000, actions_made: 1823, score: 0.0\n",
    "game: 4777/10000, actions_made: 1770, score: 2.0\n",
    "game: 4778/10000, actions_made: 1996, score: 0.0\n",
    "game: 4781/10000, actions_made: 1943, score: 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el agente aprendió poco a poco haciendo juegos cada vez más largos y en su mejor juego hizo un resultado de 10 puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A continuación se explicará el funcionamiento del contenido de *Model.py*\n",
    "\n",
    "Todo está contenido dentro de una clase llamada *Model* cuyo constructor es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, actions, gamma=.1, epsilon=1, epsilon_decay=.9995, min_epsilon=.1, session=None, learning_rate=.1):\n",
    "\n",
    "        self.actions = [x for x in range(len(actions))]\n",
    "        # exploit vs explore value\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # deep learning neural network attributes\n",
    "        self.gamma = gamma\n",
    "        self.actions_made = []\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_model()\n",
    "\n",
    "\n",
    "        if session is None:\n",
    "            self.session = tf.InteractiveSession()\n",
    "        else:\n",
    "            self.session = session\n",
    "        self.session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden distinguir las siguientes inicializaciones:\n",
    "* El conjunto de acciones posibles es un parámetro obligatorio del constructor y se guardan como una lista de índices.\n",
    "* Los parámetros que controlan la razón entre explorar nuevos valores y utilizar lo ya aprendido.\n",
    "* Los parámetros relacionados con el proceso de aprendizaje de la red neuronal.\n",
    "* La sesión de Tensorflow.\n",
    "\n",
    "Cabe mencionar que el modelo en sí se construye en la línea *self.model = self._build_model()* done la función invocada es como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _build_model(self):\n",
    "        # first layer\n",
    "        self.xT = tf.placeholder(tf.uint8, shape=(None, 82, 75, 3), name=\"x\")\n",
    "        self.yT = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "        self.actionsT = tf.placeholder(tf.int32, shape=(None), name=\"actions\")\n",
    "\n",
    "        # x_normalized = tf.to_float(x) / 255.0\n",
    "        X = tf.to_float(self.xT) / 255.0\n",
    "        batch_size = tf.shape(X)[0]\n",
    "\n",
    "        # 3 conv networks\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # 2 fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        predictions = tf.contrib.layers.fully_connected(fc1, len(self.actions))\n",
    "\n",
    "        # optimizer for the nn\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(predictions)[1] + self.actionsT\n",
    "        self.action_predictions = tf.gather(tf.reshape(predictions, [-1]), gather_indices)\n",
    "        \n",
    "        self.losses = tf.squared_difference(self.yT, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "\n",
    "        self.train_op = optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo los resultados obtenidos en el artículo \"Playing Atari With Deep Reinforcement Learning\", la arquitectura es como sigue:\n",
    "* Tres capas convolucionales: 32 filtros de 8x8, 64 filtros de 4x4 y 64 filtros de 3x3\n",
    "* Dos capas totalmente conexas: 1 capa oculta y 1 capa de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función principal de la clase *Model* es la función *run()* que precisamente ejecuta el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def run(self, games, env):\n",
    "        max_reward = 0\n",
    "        max_game = 0\n",
    "        max_actions_made = 0\n",
    "        for game in range(games):\n",
    "            total_reward = 0\n",
    "            state = self.preprocess_image(env.reset())\n",
    "            while True:\n",
    "                # get the next action\n",
    "                action = self.explore_or_exploit(state)\n",
    "\n",
    "                # act\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                state = self.preprocess_image(next_state)\n",
    "                self.save_current_state(state, action, total_reward, self.preprocess_image(next_state), done)\n",
    "                # get next_state\n",
    "\n",
    "                # if finished, tell me it finished\n",
    "                if done:\n",
    "                    if total_reward >= max_reward:\n",
    "                        max_game = game\n",
    "                        max_actions_made = len(self.actions_made)\n",
    "                        max_reward = total_reward\n",
    "                    print('game: {}/{}, actions_made: {}, score: {}'.format(game, games, len(self.actions_made), total_reward))\n",
    "                    break\n",
    "\n",
    "            self.fit_nn()\n",
    "            self.actions_made = []\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay if self.epsilon * self.epsilon_decay > self.min_epsilon else self.min_epsilon\n",
    "        print(\"\\n\\nBest game {}, actions_made: {}, score {}\".format(max_game, max_actions_made, max_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso general se puede ver de la siguiente manera:\n",
    "    \n",
    "Para cada juego que se vaya a jugar:\n",
    "1. Reinicia el simulador.\n",
    "2. Escoge una acción a realizar.\n",
    "3. Simula el juego con esa acción y obtiene una observación.\n",
    "4. La observación es preprocesada.\n",
    "5. Se guarda el estado actual del juego incluyendo: imagen preprocesada, acción realizada, recompensa total, y una bandera indicando si el juego llegó a un punto de terminación.\n",
    "6. Si el juego ha terminado se imprimen los resultados.\n",
    "\n",
    "Al terminar de jugar el juego y antes de empezar el siguiente:\n",
    "1. Actualiza los pesos de la red neuronal al llamar a la función fit_nn().\n",
    "2. Limpia la lista de acciones realizadas.\n",
    "3. Actualiza el *epsilon* que se utiliza para decidir entre explorar y explotar conocimiento.\n",
    "\n",
    "De las funciones invocadas las más importantes se muestran a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función *explore_or_exploit* se encarga de decidir si al escoger una acción el modelo va a explorar agarrando de forma aleatoria la función o si va a aprovechar el conocimiento ya obtenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def explore_or_exploit(self, state):\n",
    "        if np.random.binomial(1, self.epsilon):\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            action = np.argmax(self.predict(state)[0])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función *fit_nn()* se encarga de entrenar la red con las acciones y los resultados obtenidos en el último juego. En ella se puede observar que sólo se toma el 10% de las acciones realizadas durante el juego, las cuales se escogen con un muestreo aleatorio.\n",
    "\n",
    "Para cada acción muestreada se define el valor objetivo y se actualizan los pesos de la red al llamar a la función *self.update(states, actionsT, targets)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit_nn(self):\n",
    "        batch_size = int(len(self.actions_made)*.1) # 10% of actions made\n",
    "        indexes = np.random.randint(0, len(self.actions_made), batch_size)\n",
    "        actions_sampled = [self.actions_made[x] for x in indexes]\n",
    "        states = []\n",
    "        actionsT = []\n",
    "        targets = []\n",
    "\n",
    "        for state, action, reward, next_state, done in actions_sampled:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.argmax(self.predict(next_state))\n",
    "            \n",
    "            states.append(state)\n",
    "            actionsT.append(action)\n",
    "            targets.append(target)\n",
    "            \n",
    "        self.update(states, actionsT, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El contenido de la función update es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update(self, s, a, y):\n",
    "        states_reshaped = np.array(s).reshape(len(s), 82, 75, 3)\n",
    "        feed_dict = { self.xT: states_reshaped, self.yT: y, self.actionsT: a }\n",
    "\n",
    "        loss, _ = self.session.run([self.loss, self.train_op], feed_dict=feed_dict)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nótese que se invoca a *self.loss* y a *self.train_op* que se encargan de calcular el error y de entrenar la red a partir del mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución en Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar el código en una instancia con GPU en Amazon Azure se utilizó el código siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from Model import Model\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "env.reset()\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as session:\n",
    "    modelo = Model(env.unwrapped.get_action_meanings(), session)\n",
    "    saver = tf.train.Saver()\n",
    "    modelo.run(10000, env)\n",
    "    save_path = saver.save(session, 'saved_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código agrega directivas de configuración para el uso de GPU y al final guarda el modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
